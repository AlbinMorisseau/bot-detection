# D√©tection de Bots √† partir de Logs HTTP avec XGBoost

Ce projet a pour objectif de construire un mod√®le de machine learning capable de diff√©rencier le trafic g√©n√©r√© par des humains de celui g√©n√©r√© par des bots, en se basant sur l'analyse de logs du moteur de recherche de la biblioth√®que et centre d‚Äôinformation de l‚ÄôUniversit√© Aristote de Thessalonique, en Gr√®ce. (https://zenodo.org/records/3477932)

Les journaux de serveur obtenus couvrent un mois complet, du 1er au 31 mars 2018, et comprennent 4‚ÄØ091‚ÄØ155 requ√™tes, avec une moyenne de 131‚ÄØ973 requ√™tes par jour et un √©cart-type de 36‚ÄØ996,7 requ√™tes. Au total, les requ√™tes proviennent de 27‚ÄØ061 adresses IP uniques et de 3‚ÄØ441 cha√Ænes user-agent distinctes.

J'ai utilis√© une version trait√©e des journaux de logs, sous forme d‚Äôun dataset √©tiquet√©, o√π les entr√©es sont regroup√©es en sessions et accompagn√©es de leurs caract√©ristiques extraites.

# Description des features du dataset initial (interpr√©tation personnelle)

1. **ID**

Identifiant unique de la session ou de l‚Äôutilisateur.

2. **NUMBER_OF_REQUESTS**

Nombre total de requ√™tes effectu√©es dans la session.

Plus ce nombre est √©lev√©, plus la session est active ; peut indiquer un robot si c‚Äôest tr√®s √©lev√©.

3. **TOTAL_DURATION**

Dur√©e totale de la session en secondes.

Permet de voir combien de temps la session a dur√© sur le serveur.

4. **AVERAGE_TIME**

Dur√©e moyenne par requ√™te.

Les bots peuvent avoir des temps tr√®s courts ou tr√®s constants.

5. **STANDARD_DEVIATION**

√âcart-type du temps entre les requ√™tes.

Faible √©cart-type peut indiquer un comportement automatique (robot), tr√®s r√©gulier.

6. **REPEATED_REQUESTS**

Fraction de requ√™tes r√©p√©t√©es dans la session.

Les bots font souvent des requ√™tes r√©p√©titives.

7. **HTTP_RESPONSE_2XX**

Proportion de r√©ponses 2XX (succ√®s) dans la session.

8. **HTTP_RESPONSE_3XX**

Proportion de r√©ponses 3XX (redirections).

9. **HTTP_RESPONSE_4XX**

Proportion de r√©ponses 4XX (erreurs c√¥t√© client).

10. **HTTP_RESPONSE_5XX**

Proportion de r√©ponses 5XX (erreurs c√¥t√© serveur).

11. **GET_METHOD**

Fraction de requ√™tes GET.

12. **POST_METHOD**

Fraction de requ√™tes POST.

13. **HEAD_METHOD**

Fraction de requ√™tes HEAD.

14. **OTHER_METHOD**

Fraction de requ√™tes avec d‚Äôautres m√©thodes HTTP.

15. **NIGHT**

Proportion de requ√™tes faites pendant la nuit.

Les robots peuvent avoir un usage uniforme 24/7 contrairement aux humains (la majorit√© du moins ).
ON suppose que cela est call√© sur le fuseau horaire de Gr√®ce

16. **UNASSIGNED**

Proportion de requ√™tes non class√©es ou ind√©termin√©es.

17. **IMAGES**

Proportion de requ√™tes pour des images.

18. **TOTAL_HTML**

Proportion de requ√™tes pour des fichiers HTML.

19. **HTML_TO_IMAGE**

Ratio HTML / Images.

Peut indiquer si la session charge principalement des pages ou des ressources.

20. **HTML_TO_CSS**

Ratio HTML / CSS.

21. **HTML_TO_JS**

Ratio HTML / JavaScript.

22. **WIDTH**

Nombre moyen de liens ou de branches dans le site explor√© par session.

23. **DEPTH**

Profondeur maximale atteinte dans la navigation du site.

24. **STD_DEPTH**

√âcart-type de la profondeur des pages visit√©es.

25. **CONSECUTIVE**

Fraction ou nombre de requ√™tes cons√©cutives.

Indique des comportements syst√©matiques ou automatiques.

26. **DATA**

Volume total de donn√©es transf√©r√©es (octets).

27. **PPI**

Possiblement ‚Äúpages per interval‚Äù ou un indicateur de fr√©quence de requ√™tes.

28. **SF_REFERRER**

Fraction de requ√™tes ayant un referrer sp√©cifique.

29. **SF_FILETYPE**

Fraction de types de fichiers sp√©cifiques (HTML, CSS, JS, images).

30. **MAX_BARRAGE**

Nombre maximum de requ√™tes cons√©cutives dans un intervalle tr√®s court.

31. **PENALTY**

Score ou indicateur de comportement suspect probablement calcul√© par un algorithme interne.

32. **ROBOT**

Label cible : 1 si la session est identifi√©e comme robot, 0 si humain.

## Table des Mati√®res
1. [Contexte du Projet](#contexte-du-projet)
2. [Set up du projet](#set-up-du-projet)
3. [Structure du Projet](#structure-du-projet)
4. [Workflow](#workflow)
5. [R√©sultats](#r√©sultats)
6. [Conclusion et Pistes d'Am√©lioration](#conclusion-et-pistes-dam√©lioration)

## Contexte du Projet

Les bots malveillants repr√©sentent une **menace** croissante pour les infrastructures web, notamment dans les secteurs du e-commerce et du luxe. Selon le Global Bot Security Report 2024 de l'entreprise DataDome, 95‚ÄØ% des bots avanc√©s passent inaper√ßus, ce qui expose les sites √† des risques tels que le vol de contenu, le scraping de donn√©es sensibles et des attaques par d√©ni de service distribu√© (DDoS) 
De plus, 60‚ÄØ% des sites n'ont mis en place aucune protection contre les attaques basiques de bots.

Dans ce contexte, l'objectif de ce projet est de **d√©velopper des m√©thodes d'analyse permettant d'identifier et de diff√©rencier les comportements des utilisateurs humains de ceux des robots.** Une telle approche est essentielle pour prot√©ger les ressources informatiques, pr√©server l'int√©grit√© des donn√©es et optimiser l'exp√©rience utilisateur en filtrant les interactions non humaines.

## Set up du projet

1.  **Clonez le d√©p√¥t :**
    ```bash
    git clone [https://github.com/AlbinMorisseau/bot-detection.git](https://github.com/AlbinMorisseau/bot-detection.git)
    cd bot-detection-project
    ```
2.  **Cr√©ez et activez un environnement virtuel :**
    ```bash
    python -m venv venv
    # Sur Windows
    .\venv\Scripts\activate
    # Sur MacOS/Linux
    source venv/bin/activate
    ```
3.  **Installez les d√©pendances :**
    ```bash
    pip install -r requirements.txt
    ```

## Structure du Projet
```
bot-detection-project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ data.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ EDA.ipynb
‚îú‚îÄ‚îÄ src/
|   ‚îú‚îÄ‚îÄ results/
|       ‚îú‚îÄ‚îÄ 10_best_features.jpg
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix.jpg
|       ‚îú‚îÄ‚îÄ PR_curve.jpg
‚îÇ       ‚îî‚îÄ‚îÄ ROC_curve.jpg
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py
|   ‚îú‚îÄ‚îÄ preprocess.py
‚îÇ   ‚îî‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```   
## Workflow

Un premier travail de Exploratory Data Analysisa √©t√© effectu√© dans le notebook EDA.ipynb o√π on a √©tudi√© notre dataset :

- Identifier la distribution de la variable cible (ROBOT),

- D√©tecter les colonnes redondantes, corr√©l√©es ou peu informatives,

- Analyser les valeurs manquantes et les doublons,

- Comprendre les variables les plus discriminantes entre les comportements humains et automatis√©s.


Le pipeline de d√©tection de bots repose sur deux √©tapes principales : le pr√©traitement des donn√©es et l‚Äôentra√Ænement du mod√®le XGBoost avec optimisation automatique via Optuna.

1. **Pr√©traitement des donn√©es (preprocess.py)**

Suppression de colonnes non pertinentes ou limitant la g√©n√©ralisation sur de futures donn√©es

Supression des colonnes avec beaucoup de valeur manquantes car les tentatives d'imputation
ont √©t√© moins efficaces (m√©diane, moyenne,KNN)

Imputation des valeurs manquantes num√©riques (par robustesse si le dataset d'entr√©e est modifi√©)

Suppression des doublons pour √©viter les biais d‚Äôapprentissage.

D√©coupage du jeu de donn√©es en train/test (80/20), en conservant la proportion des classes.


2. **Entra√Ænement et optimisation (train.py)**

On a choisi un mod√®le XGBoost qui a fait ses preuve pour la classification binaire. Il faudra par la suite tester d'autres algorithmes de Machine Learning pour voir si on peut am√©liorer la pipeline.

Les hyperparam√®tres sont automatiquement optimis√©s gr√¢ce √† Optuna afin de maximiser le PR-AUC particuli√®rement adapt√©e aux probl√®mes de classes d√©s√©quilibr√©es.

**Hyperparam√®tres optimis√©s**

| Param√®tre | Description | Impact sur le mod√®le |
|-----------|-------------|--------------------|
| n_estimators | Nombre total d‚Äôarbres construits par le mod√®le. | Un nombre trop faible peut sous-apprendre, trop √©lev√© peut entra√Æner du sur-apprentissage. |
| max_depth | Profondeur maximale des arbres. | Contr√¥le la complexit√© : des arbres plus profonds capturent plus de relations mais risquent le sur-apprentissage. |
| learning_rate | Taux d‚Äôapprentissage pour la mise √† jour des poids. | Plus le taux est petit, plus l‚Äôapprentissage est progressif et stable. |
| subsample | Proportion d‚Äô√©chantillons utilis√©e pour chaque arbre. | Introduit de la variance, r√©duit le risque de sur-apprentissage. |
| colsample_bytree | Proportion de features utilis√©es pour chaque arbre. | Encourage la diversit√© entre arbres et am√©liore la g√©n√©ralisation. |
| gamma | Gain minimal requis pour cr√©er une nouvelle division. | R√©gularise le mod√®le en limitant les splits non significatifs. |
| lambda | Terme de r√©gularisation L2 sur les poids. | Stabilise les coefficients et √©vite le sur-ajustement. |
| alpha | Terme de r√©gularisation L1 sur les poids. | Encourage la parcimonie, favorise la s√©lection de variables. |
| scale_pos_weight | Poids relatif de la classe minoritaire. | Corrige le d√©s√©quilibre entre les classes (humain vs robot). |

3. **√âvaluation et r√©sultats**

Le mod√®le final est √©valu√© sur le jeu de test √† l‚Äôaide de plusieurs indicateurs et visualisations :

Rapport de classification (Pr√©cision, Rappel, F1-score)

Matrice de confusion

Courbes ROC et Precision-Recall

Importance des variables

Les graphiques sont enregistr√©s dans le dossier 'results'

## R√©sultats

Voici un r√©sum√© des r√©sultats obtenus pour ce travail de d√©tections de bots.
Ces premiers r√©sultats vont servir de fondements pour essayer d'am√©liorer certains aspects par la suite.

---

### 1. Choix des M√©triques

Dans le contexte de la d√©tection de bots, **l‚ÄôAccuracy (Exactitude)** n‚Äôest pas la m√©trique la plus fiable, car le jeu de donn√©es est **d√©s√©quilibr√©** (majorit√© de sessions humaines).  
Un mod√®le pourrait √™tre pr√©cis √† 95% en classant tout comme *¬´ Humain ¬ª*, si les bots ne repr√©sentent que 5% du trafic.

Voici les m√©triques que nous allons privil√©gier:

- **Rappel (Recall)** : proportion de bots correctement identifi√©s  
  $$ Recall = \frac{TP}{TP + FN} $$  
  ‚û§ *Objectif : maximiser le rappel pour minimiser les faux n√©gatifs (bots non d√©tect√©s).*

- **Pr√©cision (Precision)** : proportion de pr√©dictions "Robot" r√©ellement correctes  
  $$ Precision = \frac{TP}{TP + FP} $$  
  ‚û§ *Objectif : maintenir une bonne pr√©cision pour √©viter de bloquer des utilisateurs humains.*

- **F1-Score** et **PR AUC** (Area Under the Precision-Recall Curve) combinent ces deux aspects.  
  La **PR AUC** a √©t√© utilis√©e comme **m√©trique principale pour l‚Äôoptimisation via Optuna**.

---

### 2. Optimisation des Hyperparam√®tres avec Optuna

Une recherche automatis√©e d‚Äôhyperparam√®tres a √©t√© r√©alis√©e avec **Optuna**, en maximisant la **PR AUC** sur l‚Äôensemble de validation.

**Meilleur PR AUC (Validation) : 0.9488**

**Hyperparam√®tres optimaux :**

| Hyperparam√®tre | Valeur | Description |
| :--- | :--- | :--- |
| `n_estimators` | 807 | Nombre d‚Äôarbres de d√©cision |
| `max_depth` | 9 | Profondeur maximale des arbres |
| `learning_rate` | 0.2494 | Taux d‚Äôapprentissage |
| `subsample` | 0.9803 | Fraction d‚Äô√©chantillons utilis√©e par arbre |
| `colsample_bytree` | 0.7125 | Fraction de features utilis√©e par arbre |
| `gamma` | 0.3269 | Seuil de perte minimale pour une division |
| `lambda (L2)` | 4.8422 | Terme de r√©gularisation L2 |
| `alpha (L1)` | 1.2015 | Terme de r√©gularisation L1 |

---

### 3. R√©sultats Finaux sur le Jeu de Test

| Classe | Pr√©cision | Rappel | F1-Score | Support |
| :--- | :---: | :---: | :---: | :---: |
| **Humain** | 0.99 | 0.95 | 0.97 | 9839 |
| **Robot** | 0.75 | 0.95 | **0.84** | 1507 |
| **Accuracy globale** |  |  | 0.95 | 11346 |

**Interpr√©tation :**
- üîπ **Rappel (Bots) = 95%** ‚Üí excellente d√©tection des menaces.  
- üî∏ **Pr√©cision (Bots) = 75%** ‚Üí 25% de faux positifs (humains mal class√©s).  
  Cela reste acceptable, mais repr√©sente **l‚Äôaxe principal d‚Äôam√©lioration**.

---

### üìâ 4. Courbes de Performance

#### ‚Ä¢ Courbe ROC (Receiver Operating Characteristic)
![Courbe ROC](src/results/ROC_curve.jpg)  

**AUC = 0.99069** ‚Üí excellente capacit√© discriminatoire.

#### ‚Ä¢ Courbe Pr√©cision‚ÄìRappel
![Courbe Precision-Recall](src/results/PR_curve.jpg)

La courbe montre un maintien √©lev√© de la pr√©cision m√™me pour un rappel fort, validant la robustesse du mod√®le.

---

### üîç 5. Explicabilit√© : Importance des Caract√©ristiques

![10 most important features in average](src/results/10_best_features.jpg)

Le mod√®le identifie les **10 features les plus importantes** pour distinguer les bots des humains :

| Rang | Feature | Importance (Gain) | Interpr√©tation |
| :---: | :--- | :---: | :--- |
| 1 | `MAX_BARRAGE` | 0.5196 | Indicateur d‚Äôagressivit√© (rafales de requ√™tes) |
| 2 | `HTML_TO_CSS` | 0.1423 | Ratio de requ√™tes CSS/HTML (souvent faible pour les scrapers) |
| 3 | `DEPTH` | 0.0628 | Profondeur de navigation (anormale pour bots) |
| 4 | `REPEATED_REQUESTS` | 0.0447 | Taux d‚Äôactivit√© r√©p√©titive |
| 5 | `HTML_TO_IMAGE` | 0.0316 | Ratio images/HTML (bots ignorent souvent les images) |
| 6 | `NUMBER_OF_REQUESTS` | 0.0283 | Volume total de la session |
| 7 | `TOTAL_HTML` | 0.0253 | Nombre de fichiers HTML demand√©s |
| 8 | `GET_METHOD` | 0.0195 | Usage de la m√©thode HTTP GET |
| 9 | `TOTAL_DURATION` | 0.0163 | Dur√©e totale de la session |
| 10 | `IMAGES` | 0.0130 | Nombre de fichiers image charg√©s |

![Importance SHAP](src/results/SHAP_features_importance_detailled.png)

**Conclusion :**  
Le mod√®le a appris que les **bots** se distinguent principalement par :
- une **vitesse excessive** (`MAX_BARRAGE`),  
- un **volume important de requ√™tes** (`NUMBER_OF_REQUESTS`),  
- et des **patterns non-humains** dans les ressources demand√©es (`HTML_TO_CSS`, `HTML_TO_IMAGE`).

---

## Conclusion et pistes d'am√©liorations

Le projet d√©montre qu'un mod√®le XGBoost bien optimis√© permet de **d√©tecter efficacement les bots** √† partir de logs HTTP, avec un **rappel √©lev√© (95%)** pour la classe bot, garantissant la capture de la majorit√© des activit√©s automatis√©es. Une tentative d'explicabilit√© des features souligne que les comportements de bots se traduisent par des **rafales de requ√™tes**, des **patterns de navigation atypiques** et des **ratios de ressources sp√©cifiques (images, CSS)**.

#### Limites et axes d‚Äôam√©lioration

- **Pr√©cision des bots (75%)** : r√©duire les faux positifs pour √©viter de bloquer des utilisateurs l√©gitimes, √©ventuellement via des techniques de r√©√©chantillonnage ou de pond√©ration de classes plus fine.
- **Exploration d‚Äôautres mod√®les** : tester des approches bas√©es sur des r√©seaux de neurones ou des for√™ts al√©atoires pour comparer la performance.
- **D√©tection en temps r√©el** : adapter le pipeline pour une application en ligne, ce qui n√©cessite un traitement rapide et efficace des logs entrants.
